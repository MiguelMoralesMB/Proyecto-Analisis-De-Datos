import pandas as pd
import numpy as np
import os
import json # Necesario para guardar el resumen estadÃ­stico
from sklearn.preprocessing import MinMaxScaler

# =============================================================================
# 1. CONFIGURACIÃ“N Y RUTAS
# =============================================================================
CARPETA_DATA = "data"
ARCHIVO_CSV = os.path.join(CARPETA_DATA, "Earthquakes_USGS.csv")
ARCHIVO_PARQUET = os.path.join(CARPETA_DATA, "terremotos_limpios.parquet")
ARCHIVO_RESUMEN = os.path.join(CARPETA_DATA, "resumen_estadistico.json") # Nuevo archivo para el resumen

if not os.path.exists(CARPETA_DATA):
    os.makedirs(CARPETA_DATA)
    
# =============================================================================
# 2. FUNCIONES DE LIMPIEZA Y ANÃLISIS DESCRIPTIVO
# =============================================================================

def optimizar_memoria(df):
    """
    Reduce el uso de memoria convirtiendo tipos de datos.
    float64 -> float32, int64 -> int32, object -> category.
    """
    start_mem = df.memory_usage().sum() / 1024**2
    print(f"   Memoria antes de optimizar: {start_mem:.2f} MB")
    
    for col in df.columns:
        col_type = df[col].dtype
        
        if col_type != object:
             # LÃ³gica de optimizaciÃ³n numÃ©rica (mantenida de tu script)
            c_min = df[col].min()
            c_max = df[col].max()
            if str(col_type)[:3] == 'int':
                if c_min > np.iinfo(np.int32).min and c_max < np.iinfo(np.int32).max:
                    df[col] = df[col].astype(np.int32)
            else:
                if c_min > np.finfo(np.float32).min and c_max < np.finfo(np.float32).max:
                    df[col] = df[col].astype(np.float32)
        else:
            # LÃ³gica de optimizaciÃ³n de texto a CategorÃ­a
            num_unique = len(df[col].unique())
            num_total = len(df[col])
            if num_unique / num_total < 0.5:
                df[col] = df[col].astype('category')

    end_mem = df.memory_usage().sum() / 1024**2
    print(f"   Memoria despuÃ©s de optimizar: {end_mem:.2f} MB")
    return df

def crear_resumen_estadistico(df):
    """
    Crea un resumen estadÃ­stico clave usando describe() y groupby()  
    y lo guarda como un archivo JSON para que Flask lo pueda leer.
    """
    print("   Generando Resumen EstadÃ­stico para Flask...")
    
    # 1. DescripciÃ³n General (describe())
    # Usamos solo los principales tipos de datos para evitar errores en JSON
    resumen_general = df.describe(include=[np.number, 'category']).transpose()
    # Convertimos a string y luego a dict, ya que .to_dict() directo puede fallar con categorÃ­as.
    resumen_dict = resumen_general.astype(str).to_dict()

    # 2. Conteo por AÃ±o/DÃ©cada (groupby())
    if 'time' in df.columns:
        # AÃ±adir columna de aÃ±o y dÃ©cada para el anÃ¡lisis temporal
        df['year'] = df['time'].dt.year
        df['decade'] = (df['year'] // 10 * 10).astype('category')
        
        # Conteo de sismos por dÃ©cada
        sismos_por_decada = df.groupby('decade')['mag'].count().to_dict()
    else:
        sismos_por_decada = {"Error": "Columna 'time' no encontrada o no convertida."}

    # 3. Conteo de paÃ­ses/regiones mÃ¡s afectadas (groupby())
    if 'place' in df.columns:
        # Encontramos la regiÃ³n mÃ¡s prominente de los 10 primeros
        conteo_regiones = df['place'].value_counts().nlargest(10).to_dict()
    else:
        conteo_regiones = {"Error": "Columna 'place' no encontrada"}

    resumen_final = {
        "general": resumen_dict,
        "sismos_por_decada": sismos_por_decada,
        "top_regiones": conteo_regiones
    }
    
    # Guardar el JSON
    with open(ARCHIVO_RESUMEN, 'w') as f:
        json.dump(resumen_final, f, indent=4) 
    
    print(f"âœ… Resumen estadÃ­stico guardado en: {ARCHIVO_RESUMEN}")


# =============================================================================
# 3. LÃ“GICA PRINCIPAL DEL ETL
# =============================================================================

if os.path.exists(ARCHIVO_PARQUET):
    # --- CAMINO RÃPIDO ---
    print(f"ðŸš€ Archivo optimizado detectado: '{ARCHIVO_PARQUET}'.")
    terremotos = pd.read_parquet(ARCHIVO_PARQUET)
    crear_resumen_estadistico(terremotos.copy()) #Crea un json
    print("âœ… Carga completada.")

else:
    # --- CAMINO LENTO (Solo la primera vez) ---
    print(f"âš ï¸ Archivo optimizado no encontrado.")
    print(f"Cargando '{ARCHIVO_CSV}' (esto tomarÃ¡ tiempo, paciencia)...")
    
    # 1. CARGA
    try:
        terremotos = pd.read_csv(ARCHIVO_CSV, low_memory=False)
    except FileNotFoundError:
        print(f"Error crÃ­tico: El archivo {ARCHIVO_CSV} no se encuentra.")
        exit()

    print("âœ… CSV Cargado. Iniciando limpieza y optimizaciÃ³n...")

    # 2. OPTIMIZACIÃ“N DE MEMORIA
    terremotos = optimizar_memoria(terremotos)

    # 3. LIMPIEZA DE DUPLICADOS
    filas_antes = len(terremotos)
    terremotos.drop_duplicates(inplace=True)
    print(f"   Duplicados eliminados: {filas_antes - len(terremotos)}")

    # 4. TRATAMIENTO DE NULOS
    # A. Eliminar nulos crÃ­ticos (magnitud y ubicaciÃ³n)
    if 'mag' in terremotos.columns:
        terremotos.dropna(subset=['mag', 'latitude', 'longitude'], inplace=True)
    
    # B. Imputar nulos numÃ©ricos restantes con la Mediana
    cols_numericas = terremotos.select_dtypes(include=['float32', 'int32', 'float64']).columns
    for col in cols_numericas:
        terremotos[col] = terremotos[col].fillna(terremotos[col].median())

    # C. Imputar nulos de texto con 'Desconocido'
    cols_texto = terremotos.select_dtypes(include=['object', 'category']).columns
    for col in cols_texto:
        if terremotos[col].dtype.name == 'category':
            # Manejar la adiciÃ³n de categorÃ­a 'Desconocido'
            if 'Desconocido' not in terremotos[col].cat.categories:
                 terremotos[col] = terremotos[col].cat.add_categories('Desconocido')
            terremotos[col] = terremotos[col].fillna('Desconocido')
        else:
            terremotos[col] = terremotos[col].fillna('Desconocido')

    # 5. FORMATOS y Consistencia
    # Convertir columna de tiempo a datetime
    if 'time' in terremotos.columns:
        terremotos['time'] = pd.to_datetime(terremotos['time'], errors='coerce')
        
    # Normalizar texto (MinÃºsculas y sin espacios extra)
    if 'place' in terremotos.columns:
        terremotos['place'] = terremotos['place'].astype(str).str.lower().str.strip()

    # 6. NORMALIZACIÃ“N (Min-Max Scaling)
    cols_a_normalizar = ['depth', 'mag'] 
    cols_existentes = [c for c in cols_a_normalizar if c in terremotos.columns]
    
    if cols_existentes:
        scaler = MinMaxScaler()
        nombres_nuevos = [f"{c}_norm" for c in cols_existentes]
        # Creamos nuevas columnas normalizadas
        terremotos[nombres_nuevos] = scaler.fit_transform(terremotos[cols_existentes])
        print("   NormalizaciÃ³n aplicada a: ", cols_existentes)

    # 7. GUARDAR PROGRESO (Parquet y Resumen EstadÃ­stico)
    crear_resumen_estadistico(terremotos.copy()) # Creamos el resumen del dataset limpio
    
    print("ðŸ’¾ Guardando resultado en formato Parquet...")
    terremotos.to_parquet(ARCHIVO_PARQUET, index=False)
    print("âœ… Â¡Proceso completado y guardado! La prÃ³xima ejecuciÃ³n serÃ¡ instantÃ¡nea.")

# =============================================================================
# FIN DEL PROCESO ETL Y PIE PARA ANÃLISIS POSTERIORES
# =============================================================================

print("\n" + "="*50)
print("ETL COMPLETADO. Los datos limpios estÃ¡n en la carpeta 'data'.")
print("Puedes continuar con el anÃ¡lisis exploratorio (Etapa 2) desde app.py.")
print("="*50)